
@article{paczolt_multiple_2015,
	title = {Multiple Mating and Reproductive Skew in Parental and Introgressed Females of the Live-Bearing Fish {{\em Xiphophorus Birchmanni}}},
	volume = {106},
	issn = {0022-1503},
	url = {https://academic.oup.com/jhered/article/106/1/57/2960035},
	doi = {10.1093/jhered/esu066},
	language = {en},
	number = {1},
	urldate = {2019-02-03},
	journal = {Journal of Heredity},
	author = {Paczolt, Kimberly A. and Passow, Courtney N. and Delclos, Pablo J. and Kindsvater, Holly K. and Jones, Adam G. and Rosenthal, Gil G.},
	month = jan,
	year = {2015},
	pages = {57--66}
}


@article{hurlbert_pseudoreplication_1984,
	title = {Pseudoreplication and the {Design} of {Ecological} {Field} {Experiments}},
	volume = {54},
	issn = {0012-9615},
	url = {http://www.esajournals.org/doi/abs/10.2307/1942661},
	doi = {10.2307/1942661},
	abstract = {Pseudoreplication is defined as the use of inferential statistics to test for treatment effects with data from experiments where either treatments are not replicated (though samples may be) or replicates are not statistically independent. In ANOVA terminology, it is the testing for treatment effects with an error term inappropriate to the hypothesis being considered. Scrutiny of 176 experimental studies published between 1960 and the present revealed that pseudoreplication occurred in 27\% of them, or 48\% of all such studies that applied inferential statistics. The incidence of pseudoreplication is especially high in studies of marine benthos and small mammals. The critical features of controlled experimentation are reviewed. Nondemonic intrusion is defined as the impingement of chance events on an experiment in progress. As a safeguard against both it and preexisting gradients, interspersion of treatments is argued to be an obligatory feature of good design. Especially in small experiments, adequate interspersion can sometimes be assured only by dispensing with strict randomization procedures. Comprehension of this conflict between interspersion and randomization is aided by distinguishing pre—layout (or conventional) and layout—specific alpha (probability of type I error). Suggestions are offered to statisticians and editors of ecological journals as to how ecologists' understanding of experimental design and statistics might be improved.  See full-text article at JSTOR},
	number = {2},
	urldate = {2015-11-08},
	journal = {Ecological Monographs},
	author = {Hurlbert, Stuart H.},
	month = jun,
	year = {1984},
	pages = {187--211}
}

@article{davies_dont_2015,
	title = {Don't let spurious accusations of pseudoreplication limit our ability to learn from natural experiments (and other messy kinds of ecological monitoring)},
	copyright = {© 2015 The Authors. Ecology and Evolution published by John Wiley \& Sons Ltd., This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.},
	issn = {2045-7758},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/ece3.1782/abstract},
	doi = {10.1002/ece3.1782},
	abstract = {Pseudoreplication is defined as the use of inferential statistics to test for treatment effects where treatments are not replicated and/or replicates are not statistically independent. It is a genuine but controversial issue in ecology particularly in the case of costly landscape-scale manipulations, behavioral studies where ethics or other concerns may limit sample sizes, ad hoc monitoring data, and the analysis of natural experiments where chance events occur at a single site. Here key publications on the topic are reviewed to illustrate the debate that exists about the conceptual validity of pseudoreplication. A survey of ecologists and case studies of experimental design and publication issues are used to explore the extent of the problem, ecologists’ solutions, reviewers’ attitudes, and the fate of submitted manuscripts. Scientists working across a range of ecological disciplines regularly come across the problem of pseudoreplication and build solutions into their designs and analyses. These include carefully defining hypotheses and the population of interest, acknowledging the limits of statistical inference and using statistical approaches including nesting and random effects. Many ecologists face considerable challenges getting their work published if accusations of pseudoreplication are made – even if the problem has been dealt with. Many reviewers reject papers for pseudoreplication, and this occurs more often if they haven't experienced the issue themselves. The concept of pseudoreplication is being applied too dogmatically and often leads to rejection during review. There is insufficient consideration of the associated philosophical issues and potential statistical solutions. By stopping the publication of ecological studies, reviewers are slowing the pace of ecological research and limiting the scope of management case studies, natural events studies, and valuable data available to form evidence-based solutions. Recommendations for fair and consistent treatment of pseudoreplication during writing and review are given for authors, reviewers, and editors.},
	language = {en},
	urldate = {2015-11-08},
	journal = {Ecology and Evolution},
	author = {Davies, G. Matt and Gray, Alan},
	month = oct,
	year = {2015},
	keywords = {Bayesian statistics, confounded effects, hypothesis formation, nesting, peer review, P-values, random effects, scientific publication, statistical population}
}

@article{gelman_beyond_2014,
	title = {Beyond Power Calculations: Assessing Type {S} (Sign) and Type {M} (Magnitude) Errors},
	volume = {9},
	issn = {1745-6916, 1745-6924},
	url = {http://pps.sagepub.com/content/9/6/641},
	doi = {10.1177/1745691614551642},
	abstract = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.},
	language = {en},
	number = {6},
	urldate = {2015-11-08},
	journal = {Perspectives on Psychological Science},
	author = {Gelman, Andrew and Carlin, John},
	month = nov,
	year = {2014},
	pmid = {26186114},
	keywords = {design calculation, exaggeration ratio, power analysis, replication crisis, statistical significance, Type M error, Type S error},
	pages = {641--651}
}

@misc{lenth_java_2006,
	title = {Java {Applets} for {Power} and {Sample} {Size} [computer software]},
	url = {http://www.stat.uiowa.edu/~rlenth/Power},
	author = {Lenth, R. V.},
	year = {2006}
}


@article{faul_statistical_2009,
	title = {Statistical power analyses using {G}*{Power} 3.1: {Tests} for correlation and regression analyses},
	volume = {41},
	issn = {1554-3528},
	shorttitle = {Statistical power analyses using {G}*{Power} 3.1},
	url = {https://doi.org/10.3758/BRM.41.4.1149},
	doi = {10.3758/BRM.41.4.1149},
	abstract = {G*Power is a free power analysis program for a variety of statistical tests. We present extensions and improvements of the version introduced by Faul, Erdfelder, Lang, and Buchner (2007) in the domain of correlation and regression analyses. In the new version, we have added procedures to analyze the power of tests based on (1) single-sample tetrachoric correlations, (2) comparisons of dependent correlations, (3) bivariate linear regression, (4) multiple linear regression based on the random predictor model, (5) logistic regression, and (6) Poisson regression. We describe these new features and provide a brief introduction to their scope and handling.},
	language = {en},
	number = {4},
	urldate = {2019-03-17},
	journal = {Behavior Research Methods},
	author = {Faul, Franz and Erdfelder, Edgar and Buchner, Axel and Lang, Albert-Georg},
	month = nov,
	year = {2009},
	keywords = {Effect Size Measure, Implicit Association Test, Linear Multiple Regression, Multiple Correlation Coefficient, Noncentrality Parameter},
	pages = {1149--1160}
}


@article{gerard_limits_1998,
	title = {Limits of {Retrospective} {Power} {Analysis}},
	volume = {62},
	issn = {0022541X},
	url = {http://www.jstor.org/stable/3802357},
	abstract = {Power analysis after study completion has been suggested to interpret study results. We present 3 methods of estimating power and discuss their limitations. We use simulation studies to show that estimated power can be biased, extremely variable, and severely bounded. We endorse the practice of computing power to detect a biologically meaningful difference as a tool for study planning but suggest that calculation of confidence intervals on the parameter of interest is the appropriate way to gauge the strength and biological meaning of study results.},
	number = {2},
	urldate = {2010-10-25},
	journal = {The Journal of Wildlife Management},
	author = {Gerard, Patrick D. and Smith, David R. and Weerakkody, Govinda},
	month = apr,
	year = {1998},
	note = {ArticleType: research-article / Full publication date: Apr., 1998 / Copyright © 1998 Allen Press},
	pages = {801--807}
}

@article{thomas_retrospective_1997,
	title = {Retrospective {Power} {Analysis}},
	volume = {11},
	issn = {0888-8892},
	url = {http://onlinelibrary.wiley.com/doi/10.1046/j.1523-1739.1997.96102.x/abstract;jsessionid=6D27EBA7A0ED9B0498B38A0F03A0D521.d02t01},
	doi = {10.1046/j.1523-1739.1997.96102.x},
	number = {1},
	urldate = {2010-10-25},
	journal = {Conservation Biology},
	author = {Thomas, Len},
	month = feb,
	year = {1997},
	pages = {276--280}
}


@book{bolker_ecological_2008,
	title = {Ecological {Models} and {Data} in {R}},
	isbn = {0-691-12522-8},
	publisher = {Princeton University Press},
	author = {Bolker, Benjamin M.},
	month = jul,
	year = {2008}
}


@article{hoenig_abuse_2001,
	title = {The {Abuse} of {Power}},
	volume = {55},
	issn = {0003-1305},
	url = {https://doi.org/10.1198/000313001300339897},
	doi = {10.1198/000313001300339897},
	abstract = {It is well known that statistical power calculations can be valuable in planning an experiment. There is also a large literature advocating that power calculations be made whenever one performs a statistical test of a hypothesis and one obtains a statistically nonsignificant result. Advocates of such post-experiment power calculations claim the calculations should be used to aid in the interpretation of the experimental results. This approach, which appears in various forms, is fundamentally flawed. We document that the problem is extensive and present arguments to demonstrate the flaw in the logic.},
	number = {1},
	urldate = {2019-03-18},
	journal = {The American Statistician},
	author = {Hoenig, John M. and Heisey, Dennis M.},
	month = feb,
	year = {2001},
	keywords = {Bioequivalence testing, Burden of proof, Observed power, Retrospective power analysis, Statistical power, Type II error},
	pages = {19--24}
}

@article{baath_state_2012,
	title = {The {State} of {Naming} {Conventions} in {R}},
	volume = {4},
	issn = {2073-4859},
	url = {https://journal.r-project.org/archive/2012/RJ-2012-018/index.html},
	language = {en},
	number = {2},
	urldate = {2021-01-14},
	journal = {The R Journal},
	author = {Bååth, Rasmus},
	year = {2012},
	pages = {74--75}
}
